{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "923b0de7-ad39-4709-aab5-04fc8fd060b7",
   "metadata": {},
   "source": [
    "## Client, Scheduler and Workers\n",
    "\n",
    "Dask functions through a distributed computing architecture consisting of three key components: ***Client, Scheduler, and Workers***.\n",
    "\n",
    "1. **Client**:\n",
    "\n",
    "- Acts as the user interface, accepting instructions from the programmer.\n",
    "- These instructions typically involve operations on Dask collections like DataFrames or Arrays.\n",
    "- The client translates these operations into a task graph, a representation of the computational steps required.\n",
    "- The client then submits the task graph to the scheduler for further processing.\n",
    "\n",
    "2. **Scheduler**:\n",
    "\n",
    "- The central coordinator that manages the task graph.\n",
    "- Analyzes the task graph, breaking it down into smaller, independent tasks.\n",
    "- Optimizes task execution by considering factors like worker availability and data locality.\n",
    "- Schedules these tasks for execution on available workers.\n",
    "- Monitors worker progress, handling failures and redistributing tasks as needed.\n",
    "- Communicates with clients, providing updates on task completion and overall progress.\n",
    "\n",
    "3. **Workers**:\n",
    "\n",
    "- The workhorses of the system, responsible for executing the individual tasks.\n",
    "- Can be local processes running on the same machine as the client or remote processes distributed across a cluster.\n",
    "- Each worker has its own memory and processing capabilities.\n",
    "- Workers receive tasks from the scheduler, execute them on their local data, and return the results.\n",
    "- May communicate with other workers to exchange data required for task completion.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/coiled/pydata-global-dask/master/images/dask-cluster.svg\"\n",
    "     width=\"75%\"\n",
    "     alt=\"Dask cluster\\\">\n",
    "\n",
    "*Source: Coiled Dask Tutorial (https://github.com/coiled/dask-mini-tutorial/tree/main)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10789de-6d8d-4d45-8561-048b5e56fa18",
   "metadata": {},
   "source": [
    "## Schedulers\n",
    "We've explored the power of dask.delayed to build task graphs, like a blueprint for your calculations. But how do we actually run these plans and get the results? Using Dask Scheduler.\n",
    "\n",
    "Dask offers different schedulers, each with its own strengths. While they all deliver the same final answer, their performance can vary. There are two main types:\n",
    "\n",
    "- **Single-Machine Schedulers**: Perfect for crunching data on a single computer with multiple cores. They're simpler to set up and ideal for smaller datasets. (Think working with a small team on a project.)\n",
    "\n",
    "- **Distributed Schedulers**: Designed to tackle massive datasets by distributing the workload across a cluster of machines. These are more complex to set up but offer significant speed boosts for big jobs. (Imagine having a whole team working on different parts of a giant puzzle simultaneously.)\n",
    "\n",
    "Choosing the right scheduler depends on the size of your data and your computational needs. This ensures you get the most efficient processing for your specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef26bd-4257-40c8-8168-ab2d7b5b171d",
   "metadata": {},
   "source": [
    "### Single-Machine Schedulers:\n",
    "These schedulers are convenient because they don't require you to install additional software or configure complex systems. They leverage the standard libraries already present in Python.\n",
    "\n",
    "Think of this as working on the problem yourself, but with a few extra hands. The scheduler acts like the project manager:\n",
    "\n",
    "- It splits the big problem into smaller tasks.\n",
    "- It assigns these tasks to different cores (like your computer's processors) available on your machine.\n",
    "- It keeps track of the progress of each task and ensures they all work together efficiently.\n",
    "\n",
    "**Scheduler Options:**\n",
    "\n",
    "Dask offers three choices for how your chores(computations) get tackled on a single machine:\n",
    "\n",
    "- **\"Threads\" Scheduler** (Default for Arrays, DataFrames, Delayed):\n",
    "\n",
    "    - This option uses multiple threads within the same process. Think of it as having several helpers (threads) working on different chores simultaneously, but they all share the same resources like memory. It's efficient for CPU-bound tasks (tasks that require a lot of processing power).\n",
    "\n",
    "- **\"Processes\" Scheduler**:\n",
    "\n",
    "    - This option uses separate processes, each with its own memory space. Imagine having multiple assistants (processes) working on different chores independently, each with their own set of tools (memory). It's suitable for tasks that can be easily divided and don't require constant communication between chores.\n",
    "\n",
    "- **\"Single-Threaded\" Scheduler** (Debugging):\n",
    "\n",
    "    - This option acts like doing all the chores yourself (in a single thread), one after another. It's not very efficient for parallel processing, but it's valuable when you need to debug your code step-by-step or analyze how long each chore takes (profiling).\n",
    "\n",
    "The best single-machine scheduler depends on the type of data you're working with (Dask Arrays, DataFrames) and the nature of your computations. Dask provides defaults for common scenarios, but you can choose the most suitable option for your specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c76029-33b6-4597-9b09-0ba7a6ed0660",
   "metadata": {},
   "source": [
    "Now let's walk through a Dask array example where we will create a large array, perform computations (add the array to its transpose element-wise) using different single-machine schedulers, and measure the time taken for each computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9054152-2e26-4b88-9479-dc5c6408263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dask array\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fa40430-8361-422c-8116-87ca03add2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here\n",
    "\n",
    "# Define a function to measure computation time with different schedulers\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a97ac2dd-f317-4e3f-962c-4c17c2dc4e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure computation time for each scheduler\n",
    "#Your code goes here\n",
    "\n",
    "# Print the results\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7df53e5-a280-458e-a26d-b3bfee651555",
   "metadata": {},
   "source": [
    "Let's perform the computations explicitly using each scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cd14f38-7dfa-4cda-968c-cec01fda07b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here\n",
    "# Threads Scheduler\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5c67d3b-8f3f-4c53-81a0-41c83a18a607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here\n",
    "\n",
    "# Processes Scheduler\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3673e6eb-8036-4f7a-8bb4-62c53754a67c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Your code goes here\n",
    "\n",
    "# Single-Threaded Scheduler\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea197b4-4ce6-4f20-b995-0ca9dd340109",
   "metadata": {},
   "source": [
    "As you can see, for performing operation in a Dask Array, the **threads** scheduler finishes ***~48%*** faster than using the **single-threaded**,and ***~93%*** faster that the **processes** scheduler. \n",
    "\n",
    "In Dask arrays, using the threads scheduler is generally preferable to both single-threaded and processes schedulers. Let's compare them:\n",
    "\n",
    "**Threads vs. Single-Threaded:**\n",
    "\n",
    "- *Utilization of CPU Cores*:\n",
    "\n",
    "    - *Threads Scheduler*:\n",
    "        - Uses multiple threads to divide computations among available CPU cores.\n",
    "        - Can increase performance by doing calculations in parallel.\n",
    "    - *Single-Threaded Scheduler*:\n",
    "        - Executes calculations sequentially in a single thread.\n",
    "        - Lacks parallelism and cannot make full use of multi-core CPUs.\n",
    "\n",
    "- *Efficiency in Computation*:\n",
    "\n",
    "    - The task of adding a matrix and its transpose benefits from parallel processing since each member in the final matrix can be calculated independently.\n",
    "    - The \"Threads\" scheduler uses parallelism to accelerate the process by distributing the workload among multiple threads.\n",
    "    - In contrast, the \"Single-Threaded\" scheduler performs calculations sequentially, which leads to higher processing times for large datasets.\n",
    "\n",
    "**Threads vs. Processes:**\n",
    "\n",
    "- *Parallelism and Shared Memory*:\n",
    "\n",
    "    - *Threads Scheduler*:\n",
    "        - Uses multiple threads within the same process.\n",
    "        - Threads share the same memory space, allowing for efficient communication and data sharing. \n",
    "    - *Processes Scheduler*:\n",
    "        - Uses separate processes, each with its own memory space.\n",
    "        - Processes have independent memory, leading to higher memory overhead and requiring inter-process communication (IPC) for data sharing.\n",
    "\n",
    "          ***Inter-Process Communication (IPC):*** Processes cannot directly access one other's memory, therefore they need to use IPC mechanisms to transfer data and coordinate actions. IPC offers a variety of ways (such as pipes, message queues, shared memory, semaphores, and sockets) for processes to exchange data and synchronize their tasks. While IPC allows processes to communicate, it adds complexity and overhead compared to threads, which may transfer data directly without such procedures.\n",
    "\n",
    "- *Efficiency in Computation*:\n",
    "    - For the given task of computing the sum of a matrix and its transpose (result = x + x.T), the operation involves numerical computations on large arrays.\n",
    "    - The \"Threads\" scheduler can parallelize these computations across multiple threads, effectively utilizing multiple CPU cores and reducing the overall computation time.\n",
    "    - In contrast, the \"Processes\" scheduler has additional overhead due to IPC and managing separate memory spaces, making it less efficient for this type of computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e8a456-832d-4fa4-a572-342c36ce1b1d",
   "metadata": {},
   "source": [
    "### Exercise 1 (Breakout rooms)\n",
    "- Create a Dask array of random integer numbers ranging from 0 to 40. This data will represent temperatures in Celsius.\n",
    "- Convert each element in the array to fahrenheit using the following:\n",
    "    - Threads Scheduler\n",
    "    - Single-Threaded Scheduler\n",
    "    - Processes Scheduler\n",
    "- Use the %%time command to compare the computation time for each scheduler.\n",
    "- Which one is better and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef7341a4-f9a8-4369-ab96-21512050bfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "#Your code goes here\n",
    "\n",
    "# Create a Dask array of random integer numbers (temperatures in Celsius)\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "082cc03e-e202-43aa-a36f-96cfad667560",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert Celsius to Fahrenheit using threads scheduler\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01bfb6d4-dab2-4631-8d78-290627a6d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert Celsius to Fahrenheit using single-threaded scheduler\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34acaf27-10d9-402c-81ee-f072fab054e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert Celsius to Fahrenheit using processes scheduler\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b062b5b7-b754-4a84-b1a0-44d185c8754a",
   "metadata": {},
   "source": [
    "### Distributed Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b36100-6d1e-42bb-a233-cd07128285b2",
   "metadata": {},
   "source": [
    "A distributed scheduler in Dask coordinates work across multiple workers (computers or processes) in a cluster. It divides the tasks among the workers, handles data transfer between them, and ensures that computations are executed in an optimal manner.\n",
    "\n",
    "Imagine you have a team of friends, each with their own computer. You can split the problem even further and solve it much faster, the distributed scheduler acts like a coordinator for this team:\n",
    "\n",
    "- It splits the big problem into even smaller tasks than the single machine scheduler.\n",
    "- It sends these tasks to different computers (workers) in your network.\n",
    "- It communicates with all the workers, collects their results, and combines them to get the final solution.\n",
    "\n",
    "**Before we start with the exercise:**\n",
    "- Open your terminal and run the following command : ***conda install -c conda-forge msgpack-python==1.0.5***\n",
    "\n",
    "Dask depends on the msgpack-python package, which includes Dask Distributed. It is used to serialize Python objects into a compact binary format, which is necessary for fast communication between different components of a distributed Dask computation, particularly when moving data across processes or machines.\n",
    "\n",
    "By providing a specific version (1.0.5) of msgpack-python with the conda install -c conda-forge msgpack-python==1.0.5 command, you ensure that your Dask Distributed environment uses that version. This can help ensure compatibility with certain versions of Dask or solve compatibility concerns with later versions of msgpack-python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d045e208-3236-473a-b3e0-b4ba32522bdd",
   "metadata": {},
   "source": [
    "Lets first import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20b7a329-f12b-4e4a-93f7-a06980c2b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b89658-fe63-4005-9264-25ea98ed7e55",
   "metadata": {},
   "source": [
    "We start by setting up a Dask cluster on our local machine. Think of this cluster as a team of workers ready to handle our computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97389464-0880-4671-b6d5-98edc2f1b1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a local Dask cluster\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bd6f92-4be7-40e4-aa6b-5864de16862b",
   "metadata": {},
   "source": [
    "Now, lets create Dask client and connect it to the cluster. This client will be our interface for managing and assigning tasks to the workers in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dce310bf-4982-48c6-8be3-7b385f9faef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect a client to the cluster\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e84a8e-cdf3-4203-9561-afca1ac51e42",
   "metadata": {},
   "source": [
    "When you return the client object, it displays essential information about the Dask client and its connection to the cluster. This includes a unique client ID for identification, details about the connection method (like whether it's connected directly or via a cluster object), the type of cluster being used (such as a local cluster or a distributed cluster), and the URL for accessing the Dask dashboard to monitor cluster performance. \n",
    "\n",
    "Additionally, it provides information about the cluster's configuration, such as the number of workers, total memory available, scheduler details, and worker information. This summary helps users understand the setup and status of their Dask client and cluster for effective distributed computing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503639f0-27f7-4ac2-8f83-3daae540db7a",
   "metadata": {},
   "source": [
    "The next step is to create a dask array with random values of size (1000,1000) and see how the **Task Stream** graph changes in the Diagnosis Dashboard when we modify the size of the array chunks and perform operations on the array. \n",
    "\n",
    "The **Task Stream** shows the individual tasks for accross multiple threads. In this case, we have 12 total amount of threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f5d50c7-899a-4721-944d-1f5bbfcdffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dask array with random values\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0026728d-268e-47fb-9222-785f72616adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab04803-8123-4795-9f66-4eb95ac34782",
   "metadata": {},
   "source": [
    "By reading the ***Task Stream*** in our Client Dashboard we can see the number of threads (counting the rectangles down) and how as we change the chunk size of our array, the number of tasks in each thread increases.\n",
    "\n",
    "*Learning how to read the Dashboard Diagnostics goes beyond the scope of this class, but if you are curious and interested, you can checkout the resource below:*\n",
    "- Dask Dashboard Diagnostics¶ = https://docs.dask.org/en/stable/dashboard.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d210dd-94e3-46a7-bcf1-d799e2a0d779",
   "metadata": {},
   "source": [
    "Now, lets perform some statistical operations on our array, such as finding the mean, standard deviation, sum, and maximum value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05cd576f-2285-44f3-91df-3457cf494995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform statistical and mathematical operations\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7e66fb-5344-49c4-93c4-a94d4057e273",
   "metadata": {},
   "source": [
    "If we go and check the **Task Stream** we will see that it has not changed, this means that additional tasks has not been added or removed from the threads in our local Cluster. \n",
    "\n",
    "**Why do you think this is?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d9c83cc-fd00-4f32-b167-9490c397b36c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the results of the operations\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59543aee-2308-45f5-959f-7fb63f826b7d",
   "metadata": {},
   "source": [
    "When we print and calculate each delayed product, we can see in the *Task Stream* how many new tasks have been added to each thread in the local cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01e9123-edc4-458b-8ade-44a19bc13f3e",
   "metadata": {},
   "source": [
    "Finally, we close the client and cluster to release the resources and shut down the Dask cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b07e51e7-d6b2-446f-af0b-b79bb54f22e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the client and cluster\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1148aa5-09c7-4e57-afc1-aeb3ee47226d",
   "metadata": {},
   "source": [
    "# SIKU - Dask DataFrame\n",
    "Before we use SIKU to process an even larger DataFrame, let's do some local testing with a smaller version of the dask dataframe:\n",
    "\n",
    "- Open the Create_Data.py python script.\n",
    "- Find the variables ***num_files*** and ***num_rows_per_file*** and change their values to 100 and 500, respectively. This will generate a flight_data folder containing 100 csv files with flight information (500 rows each).\n",
    "- Save the script and run it in the terminal.\n",
    "\n",
    "The new flights_data csv files contain new randomly generated flight information data with the following columns:\n",
    "- **flight_id:** A unique identifier for each flight.\n",
    "- **origin:** The airport code where the flight originated from (e.g., JFK, LGA, EWR).\n",
    "- **destination:** The airport code where the flight is headed to (randomly chosen from the list of airports).\n",
    "- **airline:** The airline operating the flight (randomly chosen from Delta, United, American).\n",
    "- **status:** The status of the flight, which can be 'On Time', 'Delayed', or 'Cancelled'.\n",
    "- **delay_minutes:** The delay in minutes for the flight. If the flight is 'Cancelled', the value is set to -100; otherwise, it could be 0 for no delay or a positive integer indicating the delay time.\n",
    "- **num_passengers:** The number of passengers on the flight, randomly generated within the range of 150 to 700.\n",
    "- **distance:** The distance traveled by the flight in kilometers, randomly generated within the range of 1000 to 10000.\n",
    "- **flight_duration:** The duration of the flight in minutes, randomly generated within the range of 30 to 600.\n",
    "\n",
    "\n",
    "Once the folder is created, lets do some dask delayed and dask dataframe operations:\n",
    "\n",
    "First, let's load the needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a85d929-5dcf-4b29-8fd0-aac4346aef89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daae411-616d-43f0-a1f4-40ec4b264332",
   "metadata": {},
   "source": [
    "Now, let's load the multiple datasets in the flights_data folder into a dask dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a5d35cb-7805-45ff-94ad-b594ce078d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory containing the generated flight CSV files\n",
    "#Your code goes here\n",
    "\n",
    "# Read the CSV files using Dask, with appropriate parsing options\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf2c3b0-0620-4ffb-9fe6-eb6ea2f2bf4a",
   "metadata": {},
   "source": [
    "Next, let's create a delayed function to calculate the average delay for each origin airport and airline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f15bdad6-cb74-41e6-ab1b-602e8d9b313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0242a08a-d5e2-4090-890e-03f20505e9bd",
   "metadata": {},
   "source": [
    "Let's get the list of airlines, calculate the average delay per origin for each airline, and compute the delayed results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "609f778a-e896-4d9c-903c-ef88439df74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of airlines\n",
    "#Your code goes here#Your code goes here\n",
    "\n",
    "# Calculate average delay per origin for each airline\n",
    "#Your code goes here\n",
    "\n",
    "# Compute the delayed results\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ce4dd3-49b3-403d-a5bf-2d30cd7541a5",
   "metadata": {},
   "source": [
    "We will now create subplots to visualize the average delay for each origin airport by airline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0656517a-9505-4a7a-8afb-6a3386bc66aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots to visualize the average delay for each origin airport by airline\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c692f3-6f53-4d8b-9d0d-737a09d5daee",
   "metadata": {},
   "source": [
    "### Code Explanation:\n",
    "- ***for ax, airline, avg_delay in zip(axes, airlines, average_delays):***:\n",
    "    - **axes:** This is a list of subplot axes created by plt.subplots().\n",
    "    - **airlines:** This is a list of airline names.\n",
    "    - **average_delays:** This is a list of dataframes or series containing the average delays for each airline.\n",
    "    - **zip(axes, airlines, average_delays):** This combines the axes, airline names, and average delays so that in each iteration of the loop, ax will be one subplot, airline will be one airline name, and avg_delay will be the average delay data for that airline.\n",
    "- ***avg_delay.plot(kind='bar', ax=ax, title=f'Airline: {airline}', color='skyblue')***\n",
    "    - **avg_delay.plot(kind='bar', ...)** creates a bar plot from the avg_delay data.\n",
    "    - **ax=ax** specifies that this plot should be drawn on the current subplot axis (ax).\n",
    "    - **title=f'Airline: {airline}'** sets the title of the subplot to indicate which airline's data is being displayed.\n",
    "    - **color='skyblue'** sets the color of the bars in the plot to sky blue.\n",
    "- ***Set X and Y Labels***:\n",
    "    - **ax.set_xlabel('Origin Airport')** sets the label for the x-axis to \"Origin Airport\".\n",
    "    - **ax.set_ylabel('Average Delay (minutes)')** sets the label for the y-axis to \"Average Delay (minutes)\".\n",
    "- ***Add Gridlines***:\n",
    "    - **ax.grid(axis='y')** adds horizontal gridlines to the plot, making it easier to read the y-axis values.\n",
    "- ***Rotate X-axis Tick Labels***:\n",
    "    - **ax.set_xticklabels(avg_delay.index, rotation=45)** sets the tick labels on the x-axis using the index of the avg_delay data (which represents the origin airports).\n",
    "    - **rotation=45** rotates the tick labels by 45 degrees to make them easier to read, especially if they are long or numerous. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65536f5-df59-4705-b095-60bd6e336c2c",
   "metadata": {},
   "source": [
    "Now, let's calculate some statistics of the flights dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7ca953c-15bd-4b87-89a1-11ff05213079",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate summary statistics\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44331cc9-e9ee-4a95-8b79-55a7146601b8",
   "metadata": {},
   "source": [
    "Finally, let's create some histograms to visualize the distribution of delay minutes, number of passengers, and distance in our flight dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a0c64cc-1dd8-4524-80d1-670c486854c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms for delay_minutes, num_passengers, and distance\n",
    "#Your code goes here\n",
    "\n",
    "# Plot for delay_minutes\n",
    "#Your code goes here#Your code goes here\n",
    "\n",
    "# Plot for num_passengers\n",
    "#Your code goes here\n",
    "\n",
    "# Plot for distance\n",
    "#Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62240e8-5cd7-4302-b258-8ed885639f88",
   "metadata": {},
   "source": [
    "***Since we created random data, this information may be different for you.***\n",
    "\n",
    "The first histogram displays the distribution of flight delays. The tallest bar, located at -100, represents canceled flights. This means most of the flights in our data were cancelled. Next, there's a big group of flights at 0, indicating around 125,000 flights had no delays. After that, we see fewer flights with longer delays. For example, about 50,000 flights experienced delays between 100 and 150 minutes.\n",
    "\n",
    "This histogram helps us understand how many flights were canceled, on time, or delayed by different amounts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a23978-25b8-4357-9cb9-a3c41443a731",
   "metadata": {},
   "source": [
    "In both histograms showing the number of passengers and the distance, the bars look almost the same height. This means the data is spread out evenly across the range of values we're looking at. So, there aren't any big differences in how often different numbers of passengers or distances occur. It's like having a bunch of cookies where each type is equally popular — there's no standout favorite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf29a2c-f816-4073-a511-cfb0a5d9af9d",
   "metadata": {},
   "source": [
    "## Exercise 2: Time to use Dask in Siku! (Breakout rooms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83923bcc-7b47-4200-8dc9-aabf898450ad",
   "metadata": {},
   "source": [
    "- Create a python script to be used in Siku:\n",
    "    - Copy and paste the code we developed in this Jupyter Notebook in the new python script.\n",
    "    - **Note:** When creating a plot within a script ensure you use ***plt.savefig()*** to save your figure as a png image instead of showing it using plt.show().\n",
    "- Open the Create_Data.py script and find the variables ***num_files*** and ***num_rows_per_file*** and change their values to 1000 each. This will generate a flight_data folder containing 1000 csv files with flight information (1000 rows each).\n",
    "- Move your Python script to Siku, and create a job script to submit it. This job script should run both Create_Data.py and your analysis python script. Ensure appropriate modules are loaded. \n",
    "- Determine the appropriate amount of resources to request by submitting a job and checking the usage with seff. Update your job script accordingly, and save the seff report from the optimal job to a file LastName_FirstName_seff.txt . \n",
    "**Note**: This should **NOT** exceed 4 CPUs with 4 GB each, for more than 3 hours. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
